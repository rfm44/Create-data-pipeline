#to import the DAG object
from datetime import timedelta, datetime
from airflow import DAG
#to import necessary operators
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
#to import necessary task Functions
import urllib.request
import time
import glob, os
import json

# pull course catalog pages
def catalog():
    #define pull(url) helper function
    def pull(url):
        response = urllib.request.urlopen(url).read()
        data = response.decode('utf-8')
        return data  
    #define store(data,file) helper function
    def store(data,file):
        # open the file using open() function
        file1 = open(file,'w+')
        # Attempt to write in the file
        file1.write(data)
        # closing the file
        file1.close()
        print('wrote file: ' + file)
    urls = [ 
            'http://student.mit.edu/catalog/m1a.html',
            'http://student.mit.edu/catalog/m1b.html',
            'http://student.mit.edu/catalog/m1c.html'     
    ]

    for url in urls:
        index = url.rfind('/') + 1
        #call pull function
        data=pull(url)
        file = url[index:]
        #call store function
        store(data,file)
        print('pulled: ' + file)
        print('--- waiting ---')
        time.sleep(15)

#combine all unstructured data files into one large file
def combine():
    with open('combo.txt', 'w') as outfile:
        for file in glob.glob("*.html"):
            with open(file) as infile:
                outfile.write(infile.read())
#to scrape web pages.
def titles(): 
   from bs4 import BeautifulSoup
   def store_json(data,file):
        with open(file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print('wrote file: ' + file)
   #Open and read the large html file generated by combine()
   file = open('combo.txt', 'r')
   html_file = file.read()
   #the following replaces new line and carriage return char
   html_file = html_file.replace('\n', ' ').replace('\r', '')
   #the following create an html parser
   soup = BeautifulSoup(html_file, "html.parser")
   results = soup.find_all('h3')
   titles = []
   # tag inner text
   for item in results:
       titles.append(item.text)
   store_json(titles, 'titles.json')

def clean():
    def store_json(data,file):
        with open(file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print('wrote file: ' + file) 
    with open('titles.json') as file:
       titles = json.load(file)
       # remove punctuation/numbers
       for index, title in enumerate(titles):
           punctuation= '''!()-[]{};:'"\,<>./?@#$%^&*_~1234567890'''
           translationTable= str.maketrans("","",punctuation)
           clean = title.translate(translationTable)
           titles[index] = clean
       # remove one character words
       for index, title in enumerate(titles):
           clean = ' '.join( [word for word in title.split() if len(word)>1] )
           titles[index] = clean

       store_json(titles, 'titles_clean.json')
    
def count_words():
    from collections import Counter
    def store_json(data,file):
            with open(file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
                print('wrote file: ' + file) 
    with open('titles_clean.json','r') as file:
            titles = json.load(file)
            words = []
            # extract words and flatten
            for title in titles:
                words.extend(title.split())
            # count word frequency
            counts = Counter(words)
            store_json(counts, 'words.json')

with DAG(
   "assignment",
   start_date=days_ago(1),
   schedule_interval="@daily",catchup=False,
) as dag:
   # ts are tasks
   t0 = BashOperator(
       task_id='task_zero',
       bash_command='pip install beautifulsoup4',
       retries=2
   )
   t1 = PythonOperator(
       task_id='task_one',
       depends_on_past=False,
       python_callable=catalog
   )
   t2 = PythonOperator(
       task_id='task_two',
       depends_on_past=False,
       python_callable=combine
   )
   t3 = PythonOperator(
       task_id='task_three',
       depends_on_past=False,
       python_callable=titles
   )
   t4 = PythonOperator(
       task_id='task_four',
       depends_on_past=False,
       python_callable=clean
   )
   t5 = PythonOperator(
       task_id='task_five',
       depends_on_past=False,
       python_callable=count_words
   )

   t0>>t1>>t2>>t3>>t4>>t5